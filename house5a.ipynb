{
  "metadata": {
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "codemirror_mode": "r",
      "file_extension": ".r",
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "3.3.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "Set this variable appropriately before running the rest:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "running_as_kaggle_kernel = TRUE",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "This approach uses a simple 50/50 logarithmic average of my linear predictor\nand mtyxwp's simple support vector machine (copied from \"svm_simple\" kernel).\nMy linear predictor does quite a bit of pre-processing using ordinary least squares\nas a tool and then feeds the resulting model into an ensemble of fitting methods,\nwith weights for combining the methods determined from the fit on a validation set.\n\nStart by loading libraries.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "library(plyr)\nlibrary(caret)\nlibrary(Metrics)\nlibrary(parallel)\nlibrary(doParallel)\nlibrary(ggplot2) # Data visualization\nlibrary(readr) # CSV file I/O, e.g. the read_csv function\nlibrary(Amelia)\nlibrary(mice)\nlibrary(lattice)\nlibrary(rpart)\nlibrary(xgboost)\nlibrary(e1071)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Here are the linear fitting methods.  Many of these will be excluded from the final\npredictions, if they don't seem to help with prediction performance on the validation set.\nAnd some will be excluded for computing performance reasons if running on Kaggle.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "modelnames = c(\"lars2\",     # Least Angle Regression\n               \"cubist\",    # Cubist Regression Tree\n               \"glmboost\",  # Boosted Generalized Linear Model\n               \"glmnet\",    # Generalized linear model via penalized maximum likelihood\n               \"lasso\",     # Least absolute shrinkage & selection operator (L1 penalty)\n               \"bayesglm\",  # Bayesian Generalized Linear Model\n               \"ridge\",     # Ridge Regression (L2 penalty)\n               \"xgbLinear\", # eXtreme Gradient Boosting, linear method\n               \"nnls\",      # Non-Negative Least Squares\n               \"icr\",       # Independent Component Regression\n               \"gbm\")       # Stochastic Gradient Boosting\n\ntoo_slow <- c(\"xgbLinear\", \"cubist\", \"gbm\")",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Read the data files, and make any environment-specific adjustments.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "if (running_as_kaggle_kernel) {\n  trainfile <- \"../input/train.csv\"\n  testfile <- \"../input/test.csv\"\n  keep = ! (modelnames %in% too_slow)\n  modelnames <- modelnames[ keep ]\n} else {\n  trainfile <- \"train.csv\"\n  testfile <- \"test.csv\"\n}",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "\n## Functions\n\nFor factors that have an explicit or implicit order (e.g. Poor, Fair, Good, etc.), I am going to recode them as integers at first, so the order doesn't get lost.  Also,\nwhen I call this function, I will be careful to choose the ordering that is expected\n(based on intuition, but usually obvious) to be positively associated with sale price.\nYou'll see why later.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Function to recode levels to numeric in specified order and add \".n\" to name\n\nrecode <- function( df, var, lev ) { \n  to <- as.character( 0:(length(lev)-1) )\n  newvar <- as.numeric( as.character( mapvalues(df[[var]], from=lev, to=to) ) )\n  newname <- paste0(var,\".n\")\n  df <- cbind( df, newvar )\n  names(df)[ncol(df)] <- newname\n  df[var] <- NULL\n  df\n}",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Function for initial data cleaning. (Much of this could be done more elegantly and consistently, but I confess I didn’t start out this project by making sure the data were clean, as I should have.)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "cleanData <- function( df ) {\n  # Convert pseudo-numeric \"type of dwelling\" identifier to a factor\n  df$MSSubClass <- as.factor( df$MSSubClass )\n\n  # Deal with numeric variables that have missing values\n  df$LotFrontage = as.character( df$LotFrontage )\n  df$HasLotFrontage = ifelse( df$LotFrontage==\"NA\", 0, 1 )\n  df$LotFrontage = ifelse( df$LotFrontage==\"NA\", \"0\", df$LotFrontage ) \n  df$LotFrontage = as.numeric( df$LotFrontage )\n\n  df$MasVnrArea = as.character( df$MasVnrArea )\n  df$HasMasVnr = ifelse( df$MasVnrArea==\"NA\", 0, 1 )\n  df$MasVnrArea = ifelse( df$MasVnrArea==\"NA\", \"0\", df$MasVnrArea ) \n  df$MasVnrArea = as.numeric( df$MasVnrArea )\n\n  df$GarageYrBlt = as.character( df$GarageYrBlt )\n  df$HasGarageYr = ifelse( df$GarageYrBlt==\"NA\", 0, 1 )\n  df$GarageYrBlt = ifelse( df$GarageYrBlt==\"NA\", \"0\", df$GarageYrBlt ) \n  df$GarageYrBlt = as.numeric( df$GarageYrBlt )\n\n  # Dummy for \"has basement\"\n  df$HasBasement = ifelse( df$BsmtQual==\"NA\", 0, 1 )\n\n  # Recode ordered factors as pseudo-continuous numerical variables\n  qualcats  = c( \"Po\",  \"Fa\",  \"TA\",   \"Gd\",   \"Ex\" )\n  qualcats2 = c( \"NA\",  qualcats )\n  funcats   = c( \"Sal\", \"Sev\", \"Maj2\", \"Maj1\", \"Mod\", \"Min2\", \"Min1\", \"Typ\" )\n  basecats  = c( \"NA\",  \"Unf\", \"LwQ\",  \"Rec\",  \"BLQ\", \"ALQ\",  \"GLQ\"         )\n  df <- recode( df, \"ExterCond\",    qualcats  )\n  df <- recode( df, \"ExterQual\",    qualcats  )\n  df <- recode( df, \"HeatingQC\",    qualcats  )\n  df <- recode( df, \"KitchenQual\",  qualcats  )\n  df <- recode( df, \"BsmtCond\",     qualcats2 )\n  df <- recode( df, \"FireplaceQu\",  qualcats2 )\n  df <- recode( df, \"GarageQual\",   qualcats2 )\n  df <- recode( df, \"GarageCond\",   qualcats2 )\n  df <- recode( df, \"Functional\",   funcats   )\n  df <- recode( df, \"BsmtFinType1\", basecats  )\n  df <- recode( df, \"BsmtFinType2\", basecats  )\n  df <- recode( df, \"PavedDrive\",   c(\"N\",   \"P\",      \"Y\"                     ) )                                         \n  df <- recode( df, \"Utilities\",    c(\"ELO\", \"NoSeWa\", \"NoSewr\", \"AllPub\"      ) )\n  df <- recode( df, \"LotShape\",     c(\"IR3\", \"IR2\",    \"IR1\",    \"Reg\"         ) )                                         \n  df <- recode( df, \"BsmtExposure\", c(\"NA\",  \"No\",     \"Mn\",     \"Av\",    \"Gd\" ) )\n  df <- recode( df, \"PoolQC\",       c(\"NA\",  \"Fa\",     \"TA\",     \"Gd\",    \"Ex\" ) )\n  df <- recode( df, \"GarageFinish\", c(\"NA\",  \"Unf\",    \"RFn\",    \"Fin\"         ) )\n\n  # BsmtHeight needs special treatment, since it's really a categorized continuous variable\n  from <- c(\"NA\", \"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"  ) \n  to   <- c(\"0\",  \"50\", \"75\", \"85\", \"95\", \"120\" )                                          \n  df$BsmtHeight <- as.numeric( mapvalues(df$BsmtQual, from=from, to=to) )\n  df$BsmtQual <- NULL\n\n  # Fix numeric variables that will get read as factors in test set\n  df$BsmtFinSF1 = as.numeric( as.character( df$BsmtFinSF1 ) )\n  df$BsmtFinSF1[is.na(df$BsmtFinSF1)] = mean(df$BsmtFinSF1, na.rm=TRUE)\n  df$BsmtFinSF2 = as.numeric( as.character( df$BsmtFinSF2 ) )\n  df$BsmtFinSF2[is.na(df$BsmtFinSF2)] = mean(df$BsmtFinSF2, na.rm=TRUE)\n  df$BsmtUnfSF = as.numeric( as.character( df$BsmtUnfSF ) )\n  df$BsmtUnfSF[is.na(df$BsmtUnfSF)] = mean(df$BsmtUnfSF, na.rm=TRUE)\n  df$BsmtFullBath = as.numeric( as.character( df$BsmtFullBath ) )\n  df$BsmtFullBath[is.na(df$BsmtFullBath)] = mean(df$BsmtFullBath, na.rm=TRUE)\n  df$BsmtHalfBath = as.numeric( as.character( df$BsmtHalfBath ) )\n  df$BsmtHalfBath[is.na(df$BsmtHalfBath)] = mean(df$BsmtHalfBath, na.rm=TRUE)\n  df$GarageCars = as.numeric( as.character( df$GarageCars ) )\n  df$GarageCars[is.na(df$GarageCars)] = mean(df$GarageCars, na.rm=TRUE)\n  df$GarageArea = as.numeric( as.character( df$GarageArea ) )\n  df$GarageArea[is.na(df$GarageArea)] = mean(df$GarageArea, na.rm=TRUE)\n  \n  # Fix missing values\n  df$Utilities.n[is.na(df$Utilities.n)] = 3       # Modal value\n  df$Functional.n[is.na(df$Functional.n)] = 7     # Modal value\n  df$KitchenQual.n[is.na(df$KitchenQual.n)] = 3   # Modal value\n  df$Electrical[df$Electrical==\"NA\"] = as.factor(\"SBrkr\")\n\n  # Take logarithms where appropriate\n  df$X1stFlrSF = log( df$X1stFlrSF )\n  names(df)[names(df)==\"X1stFlrSF\"] <- \"Ln1stFlrSF\"\n  df$GrLivArea = log( df$GrLivArea )\n  names(df)[names(df)==\"GrLivArea\"] <- \"LnLivArea\"\n  df$OFHEO = log( df$OFHEO )\n  names(df)[names(df)==\"OFHEO\"] <- \"LnOFHEO\"\n\n  # Normalize dependant variable (if this is a training set)  \n  if (!is.null(df$SalePrice)) {\n    df$SalePrice = log( df$SalePrice ) - df$LnOFHEO \n    names(df)[names(df)==\"SalePrice\"] <- \"RelPrice\"\n  }\n\n  df\n}",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "OK, Wait! We interrupt this function to bring you a special announcement! That last section does something non-standard. It normalizes the target variable by dividing it by a house price index. The new target variable is called “RelPrice” and will have to be un-normalized at the end. More on this issue when I get to “Analysis” section of this notebook.\n\nNow on to the fancy stuff. The following function takes a baseline OLS model, adds a factor to it, and returns the factor’s OLS coefficients, to be used later to recode the factor as a continuous variable. (Note that the factor enters into the OLS as an exhaustive set of dummies – one for every level present – because the constant term is removed.)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Function to get coefficients to be used to make factor continuous given baseline model\n\ngetCoeffs <- function( df, basemodel, factor ) {\n  mod <- paste0( basemodel, \"+\", factor, \"-1\" )\n  lm <- lm(formula=mod, data=df)\n  fnames <- grep( factor, names(lm$coefficients), fixed=TRUE )\n  lm$coefficients[fnames]\n}",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "…and the next function takes those dummy coefficients and uses them to make the factor into a continuous variable:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Function to make factor continuous (given dummy coefficients) and add \"_r\" to name\n\nmakeContinuous <- function( df, factor, coeffs ) {\n  outvar <- 0*(1:nrow(df))\n  fact <- df[[factor]]\n  for ( n in levels(fact) ) {\n     outvar[fact==n] <- coeffs[paste0(factor,n)]\n  }   \n  df <- cbind( df, outvar )  \n  names(df)[ncol(df)] <- paste0( factor, \"_r\" )\n  df[factor] <- NULL\n  df\n}",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "That’s how I process many-valued unordered factors. As for the ordered factors, I started out by making them into integers, but the following set of functions makes them more genuinely continuous, using a procedure somewhat similar to what I did with the unordered factors. These next functions also apply to variables that were originally presented as integers (e.g. 10-point scale) but don’t represent phenomena where the ratios of different values are meaningful.\n\nAs the first step, I have a function that takes an integer variable and creates an above/below dummy for each of its values.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Function to convert integer variable to above/below dummies for each of its values\n\ndummify <- function( df, var ) { \n  v <- df[[var]]\n  vals <- sort( unique(v) )\n  n <- length(vals) - 1\n  if (n==0) { return (NULL) }\n  for (i in vals[1:n]) {\n    newname <- paste0(var,\".gt\",as.character(i))\n    newvar <- as.numeric(v > i)\n    df <- cbind( df, newvar )\n    names(df)[ncol(df)] <- newname\n  }\n  df\n}",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "As the second step, I have a function that adds those dummies to the model. But it then removes any dummies whose coefficients fit with the wrong sign.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Function to add to model any of the created dummies that have positive coefficients\n\naddDummiesToModel <- function( df, var, mod ) {\n  vars <- c()\n  for (name in names(df)) {\n    if ( grepl(paste0(var,\".gt\"), name, fixed=TRUE) ) {\n      vars <- c(vars,name)\n    }\n  }\n  newmod <- paste0( mod, \" + \", paste(vars, collapse=\" + \") )\n  lmfull <- lm( formula=newmod, data=df )\n  count = length(vars)\n  for (var in vars) {\n    if ( is.na( lmfull$coefficients[var] ) ) {\n      print( paste0( \"Variable \", var, \" removed due to rank deficiency\"))\n      newmod <- gsub( paste0(\" + \",var), \"\", newmod, fixed=TRUE )\n      count = count - 1\n    }\n    else if ( lmfull$coefficients[var] < 0 ) {\n      newmod <- gsub( paste0(\" + \",var), \"\", newmod, fixed=TRUE )\n      count = count - 1\n    }\n  }\n  if (count==0) { return(NULL) }\n  newmod\n}",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The next function runs the resulting model and extracts the dummy coefficients.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Function to get coefficients to be used to make ordered variable continuous\n#   (given a model that includes any dummies found to have positive coefficients)\n\ngetOrderedCoeffs <- function( df, newmod, var ) {\n  lm1 <- lm(formula=newmod, data=df)\n  fnames <- grep( var, names(lm1$coefficients), fixed=TRUE )\n  coeffs <- lm1$coefficients[fnames]\n  names(coeffs) <- gsub( paste0(var,\".gt\"), \"\", names(coeffs), fixed=TRUE )\n  coeffs\n}",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "But it would get verbose having to run the last three functions separately, so I have another function that runs them all and checks the results.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Function to make coefficients to be used to make ordered variable continuous\n\nmakeOrderedCoeffs <- function( df, mod, var ) {\n  df <- dummify( df, var )\n  if (is.null(df)) { \n    print( paste0(\"dummify returned NULL for \", var) )\n    return(NULL) \n  }\n  mod <- addDummiesToModel( df, var, mod )\n  if (is.null(mod)) {\n    print(\"addDummiesToModel returned NULL\")\n    return(NULL) \n  }\n  coeffs <- getOrderedCoeffs( df, mod, var )\n  if (is.null(coeffs)) {\n    print(\"getOrderedCoeffs returned NULL\")\n    return(NULL) \n  }\n  coeffs\n}",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "And finally, the next function uses the resulting coefficients to make the integer variable into a continuous one. (This often has to be done separately so that coefficients generated from training data can be applied to test data.)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Function to make ordered variable continuous (given coefficients) and add \"_r\" to name\n\nmakeOrderedContinuous <- function( df, var, coeffs ) {\n  outvar <- 0*(1:nrow(df))\n  v <- df[[var]]\n  outvar <- 0\n  for (n in names(coeffs)) {\n    outvar <- outvar + ifelse( v>as.numeric(n), coeffs[n], 0 )\n  }\n  df <- cbind( df, outvar )  \n  names(df)[ncol(df)] <- paste0( var, \"_r\" )\n  df[var] <- NULL\n  df\n}",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "But even with the functions I’ve already written, the whole process can get verbose, and it may have to be done several times, so I have the following function that goes through a bunch of ordered variables and makes them all continuous. Note that, for each variable, it uses as a base model one that includes all of the previously processed variables but not the ones yet to be processed (with one exception – well, two, but the second one would have been included either way). I could have included the yet-to-be-processed variables as pseudocontinuous integers, but I chose not to. Or I could have just used the original baseline model for each variable. (This issue also came up with the many-valued factors.) It’s not clear that what I chose was the best choice. But it’s also not clear that it wasn’t the best choice.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Function to make a bunch of ordered variables continuous using functions above\n\nmakeOrderedVariablesContinuous <- function(df, mod, varlist) {\n\n  # Note: Ordered variables should be defined so that positive coefficients are expected\n\n  # Start with a baseline model.  For each ordered variable (in sequence, so it matters\n  #   how the variables are arranged in the variable list), create above/below dummies\n  #   for each level.  Delete any dummies with wrong sign.  Use OLS coefficients on\n  #   remaining dummies to define a continuous version of the ordered variable. \n  \n  # \"OverallQual\" and \"OverallCond\" need special treatment because they are already\n  #    in the baseline model so must be removed before adding corresponding dummies.\n  already_in_baseline_model <- c(\"OverallQual\", \"OverallCond\")\n  \n  orderedCoeffs <- list() # List that will contain coefficients for ordered variables\n  varsToDrop <- c() # List that will contain variable names dropped because all wrong sign\n  i <- 0 \n  for ( var in varlist ) {\n    if ( var %in% already_in_baseline_model ) {\n      mod <- gsub( paste0(\"+ \", var), \"\", mod, fixed=TRUE ) # Delete from model\n    } \n    co <- makeOrderedCoeffs( df, mod, var )\n    if ( is.null(co) ) {\n      varsToDrop <- c(varsToDrop, var)\n      df[var] <- NULL\n    }\n    else {\n      df <- makeOrderedContinuous( df, var, co )\n      mod <- paste0( mod, \" + \", var, \"_r\")\n      i <- i + 1\n      orderedCoeffs[[i]] <- co\n      names(orderedCoeffs)[[i]] <- var\n    }\n  }\n  output <- list( df, mod, orderedCoeffs, varsToDrop )\n  names(output) <- c(\"df\", \"mod\", \"coeffs\", \"drop\")\n  output\n}",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "OK, I’m finished with the “feature generation” functions, but once they’ve run there will be more data cleaning to do (which, again, one could probably do more elegantly and robustly than I did).",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Function to do final data cleaning after variables have been processed into features\n\nfinalCleaning <- function( df ) {\n\n  # Fix numeric variables with missing values in test and/or validation set\n  df$MSSubClass_r[is.na(df$MSSubClass_r)] = mean(df$MSSubClass_r, na.rm=TRUE)\n  df$Exterior1st_r[is.na(df$Exterior1st_r)] = mean(df$Exterior1st_r, na.rm=TRUE)\n  df$Exterior2nd_r[is.na(df$Exterior2nd_r)] = mean(df$Exterior2nd_r, na.rm=TRUE)\n  df$Condition2_r[is.na(df$Condition2_r)] = mean(df$Condition2_r, na.rm=TRUE)\n  \n  # Collapse sale condition categories\n  salecon <- as.character(df$SaleCondition)\n  df$SaleMisc <- ifelse( salecon==\"Family\" | salecon==\"Partial\", 1, 0 )\n  df$SaleAbnormal <- ifelse( salecon==\"Abnorml\", 1, 0 )\n  df$SaleCondition <- NULL\n\n  # Collapse sale type categories\n  st <- as.character(df$SaleType)\n  con <- c(\"Con\", \"ConLw\", \"ConLI\", \"ConLD\")\n  wd <- c(\"WD\", \"CWD\", \"VWD\")\n  df$Contract <- ifelse( st %in% con, 1, 0 )\n  df$WrntyDeed <- ifelse( st %in% wd, 1, 0 )\n  df$NewSale <- ifelse( st==\"New\", 1, 0 )\n  df$SaleType <- NULL\n\n  # Only one kind of building type seems to be different\n  df$SingleFam <- ifelse( as.character(df$BldgType)==\"1Fam\", 1, 0 )\n  df$BldgType <- NULL\n\n  # It matters if you have a garage, but this is captured by \"HasGarageYear\"\n  # It also matters if it's a real garage or just a car port, so:\n  df$CarPort <- ifelse( as.character(df$GarageType)==\"CarPort\", 1, 0 )\n  df$GarageType <- NULL\n\n  # Residential vs. nonresidential seems to be only relevant aspect of zoning\n  zo <- as.character(df$MSZoning)\n  res_zone <- c( \"FV\", \"RH\", \"RL\", \"RP\", \"RM\" )\n  df$Residential <- ifelse( zo %in% res_zone, 1, 0 )\n  df$MSZoning <- NULL\n\n  # Get rid of RoofMatl. It is an overfit dummy for one case.\n  # Earlier analysis showed all levels got OLS coefficients that were\n  # very significantly different from zero but not different from one another.\n  # \"ClyTile\" was the omitted category and was only one case.\n  df$RoofMatl <- NULL\n\n  # Get rid of MiscFeature. Per earlier analysis, it's a mess. Don't want to deal with it.\n  df$MiscFeature <- NULL\n\n  # Factors that earlier analyses didn't like and too much of a pain in the neck to keep\n  df$Fence <- NULL\n  df$RoofStyle <- NULL\n  df$Heating <- NULL\n\n  # I didn't see any residual seasonal pattern, so:\n  df$MoSold <- NULL\n\n  # These nonlinearitiesn seem to matter\n  df$LotFrontage2 <- df$LotFrontage^2\n  df$SinceRemod <- df$YrSold - df$YearRemodAdd\n  df$SinceRemod2 <- df$SinceRemod^2\n  df$YrSold <- NULL\n  df$YearRemodAdd <- NULL\n  df$BsmtFinSF1sq <- df$BsmtFinSF1^2\n\n  # The following turn out to be redundant. But may want to bring them back later.\n  df$TotalBsmtSF <- NULL\n  df$HasMasVnr <- NULL\n  df$KitchenAbvGr_r <- NULL\n  df$GarageCond.n_r <- NULL\n  \n  df\n}",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Phew! So much for data processing functions. Now on to the model fitting.\n\nThe following function fits the models using the “train” function from the “caret” package.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Function to fit formula \"fo\" using several fitting methods \"models\"\n\nfitModels <- function( df, fo, models, runParallel, seed ) {\n\n  if (runParallel) {\n    # Set up for multiple cores\n    cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS\n    registerDoParallel(cluster)\n    }\n\n  # Fit the models\n  modelfits = list()\n  for (m in models) {\n    print ( paste(\"Training model:\", m) )\n    set.seed(seed)\n    fit <- train( as.formula(fo), data=df, method=m )\n    modelfits = c(modelfits, list(fit))\n  }\n\n  if (runParallel) {\n    # Go back to sequential processing\n    stopCluster(cluster)\n    registerDoSEQ()\n  }\n  \n  names(modelfits) <- models\n\nmodelfits \n}",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "And the following function uses those fits to make predictions.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Function to make predictions given several model fits\n\nmakePredictions <- function( df, modelfits, basepred, is_test ) {\n\n  modelnames <- names(modelfits)\n  rmses <- list()\n  predicted <- list()\n  for (fi in modelfits) {\n    writeLines ( paste(\"\\n\\n\\nPredicting for model:\", fi[[1]]) )\n    p <- predict(fi, newdata=df)\n    if (!is.null(basepred)) {\n      p[is.na(p)] <- basepred[is.na(p)]\n      }\n    predicted <- c(predicted, list(p))\n    if (!is_test) {\n      rmses <- c(rmses, rmse(df$RelPrice, p))\n    }\n  }\n  names(predicted) <- modelnames\n  if (!is_test) {\n    names(rmses) <- modelnames\n    print( rmses )\n  }\n  \n  predicted\n}",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Finally, the following function uses the predictions (presumably from a cross-validation set predicted with a model fitted on a training set) to create an ensemble of fitting procedures. It starts out by including all the procedures that have been used to fit the models. To generate the first candidate for a new set of predictions, it runs a no-constant OLS with coefficients constrained to sum to one. Then, one by one, it eliminates procedures with negative coefficients, until it ends up with an OLS where all coefficients are positive. That OLS then generates the weights for the ensemble. (I guess this is what machine learning people call “stacking” an OLS on top of the other models.)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Function to choose an ensemble of models and weight them\n\nchooseEnsemble <- function( df, predicted ) {\n\n  preddf <- cbind( as.data.frame(predicted), df$RelPrice )\n  colnames(preddf) <- c(modelnames, \"actual\")\n  bestcoef = 0\n  i = 1\n\n  while ( bestcoef <= 0 ) {\n\n    # Run full equation\n    predeq <- lm(actual~.-1, data=preddf)\n    summary( predeq )\n    cof <- predeq$coefficients\n    bestmod <- names( cof[order(cof,decreasing=TRUE)] )[i]\n    i = i + 1\n\n    # Force coefficients to sum to 1\n    preddf2 <- preddf\n    predmod <- \"actual~-1\"\n    for (n in names(preddf2)) {\n      preddf2[n] <- preddf2[n] - preddf[bestmod]\n      if (!(n==\"actual\"||n==bestmod)) {\n        predmod = paste0(predmod, \"+\", n)\n      }\n    }\n\n    # Keep dropping variables until all coefficients are positive\n    eq <- lm( predmod, data=preddf2 )\n    while( min(eq$coefficients) < 0 ) {\n      dropv <- names( which.min(eq$coefficients) )\n      predmod <- gsub( paste0(\"+\",dropv), \"\", predmod, fixed=TRUE)\n      eq <- lm(predmod, data=preddf2)\n    }\n\n    # Calculate missing coefficient\n    bestcoef = 1 - sum(eq$coefficients)\n    names(bestcoef) <- bestmod\n    weights <- c(eq$coefficients, bestcoef)\n  }\n  \n  weights\n}",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We’re almost done with functions. The following is a function that I copied from mtyxwp’s “svm_simple” kernel. It deals with missing values much more elegantly than I do in my own cleaning functions. (Since I’d already written and tested them, I didn’t want to risk introducing new bugs by replacing parts of them with this, but maybe in a future version….) This function is called in the last section of this notebook, where it starts over from scratch to fit an SVM and then averages its result with the linear one.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# mtyxwp's function to deal with missing data (copied from \"svm_simple\" kernel)\n\ndeal_missing <- function(simpledf){\n  for(i in 1:ncol(simpledf))\n  {\n    u <- simpledf[,i]\n    if (is.numeric(u))\n    {\n      simpledf[is.na(u),i] <- median(simpledf[!is.na(u),i])\n    } else\n    {\n      u[is.na(u)] <- \"Not Available\"\n      simpledf[,i] <- as.factor(u)\n    }\n  }\n  return(simpledf)\n}",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "OK, so much for functions. Now on to data.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Data\n\nFor my analysis I use a price index taken from the Federal Housing Finance Agency\n(which used to be called the Office of Federal Housing Enterprise Oversight,\nand I use the old initials, mostly because I didn't know the name had changed\nwhen I first wrote this code, but I also just like the old acronym better).\nThe data come from [FHFA's website](https://www.fhfa.gov/DataTools/Downloads/pages/house-price-index.aspx),\nbut I just cut and pasted the relevant portion into my program.  It refers\nto the West North Central House Price Index, monthly from 2006 through 2010.\n(Ames, Iowa, where the competition data are from, is in that region, and the\nsales span that range of time.)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "OFHEO = c(209.32, 210.23, 211.68, 212.71, 214.37, 215.37,\n          216.37, 216.22, 215.45, 214.48, 214.73, 211.92,\n          212.23, 214.99, 215.82, 216.99, 217.89, 218.28,\n          218.69, 216.78, 217.27, 212.78, 212.72, 211.6, \n          208.58, 208.62, 209.68, 210.28, 209.78, 210.87,\n          209.68, 208.77, 206.08, 206.07, 200.51, 201.47,\n          201.78, 204.24, 201.05, 203.8,  205.1,  206.55,\n          205.27, 204.63, 203.47, 204.22, 202.74, 199.78,\n          196.35, 197.64, 198.89, 202.13, 204.25, 204.61,\n          200.13, 201.76, 198.03, 197.87, 195.11, 193.46 )\n\nYear = c( 2006, 2006, 2006, 2006, 2006, 2006,\n          2006, 2006, 2006, 2006, 2006, 2006,\n          2007, 2007, 2007, 2007, 2007, 2007,\n          2007, 2007, 2007, 2007, 2007, 2007,\n          2008, 2008, 2008, 2008, 2008, 2008,\n          2008, 2008, 2008, 2008, 2008, 2008,\n          2009, 2009, 2009, 2009, 2009, 2009,\n          2009, 2009, 2009, 2009, 2009, 2009,\n          2010, 2010, 2010, 2010, 2010, 2010,\n          2010, 2010, 2010, 2010, 2010, 2010 )\n\nMonth = c( 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n           1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n           1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n           1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n           1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12 )\nofheo <- data.frame( Month, Year, OFHEO )",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Then I merge that data with the competition data",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "rawdata <- read.csv(trainfile, na.strings=\"\")\ndata1 <- merge(rawdata, ofheo, by.x=c(\"YrSold\",\"MoSold\"), by.y=c(\"Year\",\"Month\"))\ndata1 <- cleanData( data1 )",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "...and I got me a training set\n\nMy approach was to divide these data into 3 subsets:\n\n - \"train1\"   (60%)  for  primary training\n - \"validate\" (20%)  for  cross-validation\n - \"testing\"  (20%)  for  initial testing",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "set.seed(999)\ninTrain <- createDataPartition(y=data1$RelPrice, p=0.8, list=FALSE)\ntraining <- data1[inTrain,]\ntesting <- data1[-inTrain,]\ninTrain1 <- createDataPartition(y=training$RelPrice, p=0.75, list=FALSE)\ntrain1 <- training[inTrain1,]\nvalidate <- training[-inTrain1,]",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Initial Analysis and Feature Processing of Primary Training Data\n\nI start by making a working copy of my initial training set.  (In retrospect,\nI was probably being too careful, but for now, this is the code.)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "da <- train1",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "I'm going to come clean here.  I'm convinced that house prices (even in a place\nlike Ames, Iowa that's far from the craziness in New York and Tokyo) depend heavily\non broad market conditions.  For Heaven's sake, did we not all live through 2008?\nI don't care what the training data say:  if you tried to sell a house in 2009,\neven in Iowa, you didn't sell it for the same price you could have gotten in 2006.\nIn some ad hoc exploratory analysis I ran before writing this code, the data didn't\nseem to like the OFHEO (FHFA, whatever) price index variable very much.\nBut I'm damned if I'm going to let some officious regularization procedure \ntell me it doesn't belong in the model.  Until you can show me test data where\nthe variable makes the predictions less accurate, it stays in the damn model!\n\nNonetheless, I'm going to employ regularization procedures, because that's how\nyou get good predictions.  So I'm forcing them to accept my OFHEO variable.  How?\nBut putting it on the left-hand side, in the denominator.  Of course it will also\nbe on the right-hand side, but it will need a coefficient on -1 to be effectively\nremoved from the model.  (I'm using logarithms, so it effectively has a coefficient\nof -1 on the left-hand side, which would wash out if it also has one on the right-hand\nside.)  The regularization procedures won't be able to take it out of the model,\nbecause they only affect the right-hand side.  If they take it out of the right-hand\nside, it will be reflected one-for-one in the predictions.\n\nSo here's my baseline model",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "basemod <- \"RelPrice ~ LnOFHEO + Ln1stFlrSF + LnLivArea + OverallQual + OverallCond\"",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Very simple, but it captures the most important stuff.  If I were in a big hurry,\nI'd just use this model and be done with it.  No regularization or crazy-ass support\nvector machines.  Just good, old-fashioned ordinary least squares with a small and\nobvious set of variables.  It won't win any competitions, but it will generate pretty\ngood predictions with little effort.\n\nThe following wasn't in my original code, but just to make a point, I'm going to run this with my normalization taken out:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "simple <- lm( RelPrice+LnOFHEO ~ Ln1stFlrSF + LnLivArea + OverallQual + OverallCond, da)\nsummary( simple )",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "It explains well over 3/4 of the variance.  With 4 variables!\n\nJust for the hell of it:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "rmse( validate$RelPrice+validate$LnOFHEO, predict(simple, validate) )",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Behold the power of OLS!\n\nWe can fight like feral dogs over the next 4 percentage points of RMSE,\nbut FWIW, these 4 variables do the heavy lifting.  The difference between simple OLS\nand some of the best models in this competition might not be enough to pay\nthe realtor's commission on one of the houses we're predicting.\n\nThat said, some realtors make a damn good living on those few percentage points, \nso let's continue.\n\nProcess the many-valued factors.  (Don't worry about warnings that there are\nmissing levels.)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Make factors continuous and add continuous versions to baseline model one by one\n\nfactors <- c( \"Neighborhood\", \"MSSubClass\", \"Condition1\", \"Exterior1st\", \"Condition2\", \n              \"Exterior2nd\",  \"LotConfig\",  \"Foundation\")\nmod <- basemod\ncoeffs <- list()\ni <- 0 \nfor (f in factors) {    \n  co <- getCoeffs( da, mod, f )\n  i <- i + 1\n  coeffs[[i]] <- co\n  names(coeffs)[i] <- f\n  da <- makeContinuous( da, f, co )\n  mod <- paste0( mod, \"+\", f, \"_r\" ) \n}",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We're up to 87% of the variance now (but the adjusted R-squared is not meaningful,\nsince my \"make factors continuous\" procedure stole a bunch of degrees of freedom\nand hid them under the bed)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Show output of augmented model\nprint( summary( lm( formula=mod, data=da ) ) )",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Now we make the ordered variables continuous",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "ordered <- c(\"OverallQual\",    \"OverallCond\",  # 1st 2=special cases bcuz in baseline mod  \n             \"Functional.n\", \"Fireplaces\",     \"KitchenQual.n\", \n             \"BsmtExposure.n\", \"HeatingQC.n\",  \"Utilities.n\",    \"FullBath\",  \n             \"HalfBath\",       \"GarageCars\",   \"BsmtFullBath\",   \"GarageQual.n\", \n             \"BsmtFinType1.n\", \"PavedDrive.n\", \"BsmtCond.n\",     \"GarageCond.n\", \n             \"FireplaceQu.n\",  \"ExterQual.n\",  \"TotRmsAbvGrd\",   \"LotShape.n\", \n             \"BsmtHalfBath\",   \"PoolQC.n\",     \"BsmtFinType2.n\", \"ExterCond.n\", \n             \"BedroomAbvGr\",   \"BsmtHeight\",   \"KitchenAbvGr\",   \"GarageFinish.n\") \n\nout <- makeOrderedVariablesContinuous(da, mod, ordered)\nda <- out$df\nmod <- out$mod\norderedCoeffs <- out$coeffs\nvarsToDrop <- out$drop",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Clean the data",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "da = finalCleaning( da )",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Explore the result by looking at an OLS with everything but the kitchen sink.  (Oh,\nwait, maybe there is a variable about the kitchen sink.  I'll have to check the\ndata description again.)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "regmodel = lm( formula=\"RelPrice ~ .\", data=da )\nprint( summary( regmodel ) )",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Quick and Dirty Look at Validation Set\n\nMake a working copy.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "da2 <- validate",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Apply the transformations that we got from the training set.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "for (f in factors) {    \n  co <- coeffs[[f]]\n  da2 <- makeContinuous( da2, f, co )\n}\nfor ( var in ordered ) {\n  if ( var %in% varsToDrop ) {\n    da2[var] <- NULL\n  }\n  else {\n    da2 <- makeOrderedContinuous( da2, var, orderedCoeffs[[var]] )\n  }\n}\nda2 <- finalCleaning( da2 )",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Predict using the \"Kitchen Sink\" OLS Model",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Make predictions\nprediction <- predict(regmodel, da2, type=\"response\")\n\n# Fill in missing values\nbaselm <- lm(formula=basemod, data=train1)\nbasepred <- predict( baselm, validate, type=\"response\")\nprediction[is.na(prediction)] <- basepred[is.na(prediction)]\n\n# RMSE\nrmse(da2$RelPrice,prediction)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "That's implausibly good.  Which means CARET's random assignment has given us\nan unusually easy-to-predict validation set.  (It also casts doubt on my earlier ranting about realtors and such, but I think my basic point still stands.)\n\nAlso why I should use k-fold cross-validation in the future, but I'm going to continue\nwith this one for now, since the code is already written and tested.\n\nInspecting the earlier OLS output, I threw out the variables that looked like\nthey weren't helping, and this is what I came up with:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "fo = \"RelPrice ~ LotFrontage + LotArea + Alley + LandContour + LandSlope \"\nfo = paste0(fo, \"+ YearBuilt + MasVnrArea + BsmtFinSF1 + BsmtFinSF2 \")\nfo = paste0(fo, \"+ BsmtUnfSF + CentralAir + Ln1stFlrSF + X2ndFlrSF \")\nfo = paste0(fo, \"+ LowQualFinSF + LnLivArea + GarageArea + WoodDeckSF \")\nfo = paste0(fo, \"+ OpenPorchSF + EnclosedPorch + X3SsnPorch + ScreenPorch \")\nfo = paste0(fo, \"+ LnOFHEO + HasLotFrontage + Neighborhood_r + Condition1_r \")\nfo = paste0(fo, \"+ Exterior1st_r + Condition2_r + Exterior2nd_r + LotConfig_r \")\nfo = paste0(fo, \"+ Foundation_r + OverallQual_r + OverallCond_r + Functional.n_r \")\nfo = paste0(fo, \"+ Fireplaces_r + KitchenQual.n_r + BsmtExposure.n_r + HeatingQC.n_r \")\nfo = paste0(fo, \"+ FullBath_r + HalfBath_r + GarageCars_r + BsmtFullBath_r \")\nfo = paste0(fo, \"+ GarageQual.n_r + BsmtFinType1.n_r + ExterQual.n_r + TotRmsAbvGrd_r \")\nfo = paste0(fo, \"+ PoolQC.n_r + BsmtFinType2.n_r + ExterCond.n_r + SaleAbnormal \")\nfo = paste0(fo, \"+ Contract + NewSale + SingleFam + Residential + LotFrontage2 \")\nfo = paste0(fo, \"+ SinceRemod + SinceRemod2 + BsmtFinSF1sq \")\n\nmymodel = lm( formula=fo, data=da )\nprediction <- predict(mymodel, da2, type=\"response\")\nprediction[is.na(prediction)] <- basepred[is.na(prediction)]\nrmse(da2$RelPrice,prediction)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Not a big improvement, but, whatev.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Choosing an Ensemble of Models\n\nFit the models.  (Go have lunch while this section runs, if you're using Kaggle or\nhave a slow computer.)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "modelfits <- fitModels( da, fo, modelnames, !running_as_kaggle_kernel, 998 )",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Create the ensemble.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "predicted <- makePredictions ( da2, modelfits, basepred, FALSE )\nweights <- chooseEnsemble( da2, predicted )\nbestmodels <- names(weights)\nprint( weights )",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Take a look at the ensemble results for the validation set.  (Note that these are\nin-sample predictions from the point of view of the ensemble-maker, so take with\na grain of salt.)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "preddf <- cbind( as.data.frame(predicted), da2$RelPrice )\ncolnames(preddf) <- c(modelnames, \"actual\")\np_ensemble <- as.data.frame( as.matrix(preddf[bestmodels]) %*% weights )\nnames( p_ensemble ) <- \"ensemble\"\nrmse( da2$RelPrice, p_ensemble )",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Initial Testing\n\nRedo the feature creation process on data that include the validation set",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Make working copy of data\nda3 <- training\n\n# Make factors continuous\nmod <- basemod\ncoeffs <- list()\ni <- 0 \nfor (f in factors) {    \n  co <- getCoeffs( da3, mod, f )\n  i <- i + 1\n  coeffs[[i]] <- co\n  names(coeffs)[i] <- f\n  da3 <- makeContinuous( da3, f, co )\n  mod <- paste0( mod, \"+\", f, \"_r\" ) \n}\n\n# Make ordered variables continuous\nout <- makeOrderedVariablesContinuous(da3, mod, ordered)\nda3 <- out$df\nmod <- out$mod\norderedCoeffs <- out$coeffs\nvarsToDrop <- out$drop\n\n# Make data nice\nda3 = finalCleaning( da3 )",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Process the initial test data accordingly",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Make working copy of data\nda4 <- testing\n\n# Make factors continuous and add continuous versions to baseline model one by one\nfor (f in factors) {    \n  co <- coeffs[[f]]\n  da4 <- makeContinuous( da4, f, co )\n}\n\n# Make ordered variables continuous\nfor ( var in ordered ) {\n  if ( var %in% varsToDrop ) {\n    da4[var] <- NULL\n  }\n  else {\n    da4 <- makeOrderedContinuous( da4, var, orderedCoeffs[[var]] )\n  }\n}\n\n# Make data nice\nda4 <- finalCleaning( da4 )",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Fit models.  (Go have a snack.)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "modelfits <- fitModels( da3, fo, bestmodels, !running_as_kaggle_kernel, 997 )",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Make individual model predictions.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "predicted <- makePredictions ( da4, modelfits, NULL, FALSE )",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Combine results into an ensemble prediction, and check performance.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "p_ensemble <- 0*da4$RelPrice\npreddf <- as.data.frame( predicted )\np_ensemble <- as.data.frame( as.matrix(preddf[bestmodels]) %*% weights )\nnames(p_ensemble) <- \"ensemble\"\n\n# Estimated forecast error\nprint( rmse( da4$RelPrice, p_ensemble ) )",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Not as good as I would have hoped, but maybe CARET gave us a hard-to-predict\ninitial test set, just as it gave us an easy-to-predict validation set.\nAgain, the future is k-fold.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Training the Final Model\n\nFeature creation again",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Make working copy of data\nda5 <- data1\n\n# Make factors continuous\nmod <- basemod\ncoeffs <- list()\ni <- 0 \nfor (f in factors) {    \n  co <- getCoeffs( da5, mod, f )\n  i <- i + 1\n  coeffs[[i]] <- co\n  names(coeffs)[i] <- f\n  da5 <- makeContinuous( da5, f, co )\n  mod <- paste0( mod, \"+\", f, \"_r\" ) \n}\n\n# Make ordered varibles continuous\nout <- makeOrderedVariablesContinuous(da5, mod, ordered)\nda5 <- out$df\nmod <- out$mod\norderedCoeffs <- out$coeffs\nvarsToDrop <- out$drop\n\n# Make data nice\nda5 = finalCleaning( da5 )",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Let's take a look to the kitchen sink regression for the full training set",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "regmodel = lm( formula=\"RelPrice ~ .\", data=da5 )\nprint( summary( regmodel ) )",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Based on these results, I'm going to make some changes to the feature list",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "fo <- gsub( \"+ ExterQual.n_r \", \"\", fo, fixed=TRUE  )\nfo <- gsub( \"+ ExterCond.n_r \", \"\", fo, fixed=TRUE )\nfo <- gsub( \"+ PoolQC.n_r \", \"\", fo, fixed=TRUE )\nfo <- paste0(fo, \"+ FireplaceQu.n_r + BsmtHeight_r + SaleMisc \")\n\nregmodel = lm( formula=fo, data=da5 )\nprint( summary( regmodel ) )",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Here comes the final fit.  (Time for dinner yet?)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "modelfits <- fitModels( da5, fo, bestmodels, !running_as_kaggle_kernel, 996 )",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Predicting the Test Data with Linear Model Ensemble\n\nRead in and process the test data.  (This looks easy, but it was hard\nthe first time I did it, because there was considerable untidiness in the test data\nthat I foolishly failed to anticipate.)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Read data\ntestdat <- read.csv(testfile, na.strings=\"\")\ndata2 <- merge(testdat, ofheo, by.x=c(\"YrSold\",\"MoSold\"), by.y=c(\"Year\",\"Month\"))\ndata2 <- cleanData( data2 )\n\n# Make factors continuous and add continuous versions to baseline model one by one\nfor (f in factors) {    \n  co <- coeffs[[f]]\n  data2 <- makeContinuous( data2, f, co )\n}\n\n# Make ordered variables continuous\nfor ( var in ordered ) {\n  if ( var %in% varsToDrop ) {\n    data2[var] <- NULL\n  }\n  else {\n    data2 <- makeOrderedContinuous( data2, var, orderedCoeffs[[var]] )\n  }\n}\n\ndata2 <- finalCleaning( data2 )",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Generate predictions",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "predicted <- makePredictions ( data2, modelfits, NULL, TRUE )\n\npreddf <- as.data.frame( predicted )\np_ensemble <- as.data.frame( as.matrix(preddf[bestmodels]) %*% weights )\nnames(p_ensemble) <- \"ensemble\"",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Transform predictions back into the form that Kaggle wants them",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "prediction <- p_ensemble + data2$LnOFHEO\n\nresult <- data.frame( cbind( data2$Id, exp(prediction) ) )\nnames(result) <- c(\"Id\", \"SalePrice\")\nsorted_result <- result[order(result$Id),]",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "And one could just store the results, but I'm going to buy another 40 basis points\nor so of RMSE improvement by averaging these predictions with those of a completely\ndifferent, but comparably performing, model.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Training Simple SVM and Predicting Test Data\n\nThis code is from mtyxwp's \"svm_simple\" kernel.  mtyxwp's approach to the data is\ncompletely different from mine, and the model is completely different, and\ndiversity is valuable when it comes to making predictions.  Since I didn't write\nthis code, I'm not sure I can throw much light on it, beyond what is self-explanatory.\nHere's the data processing, which makes no attempt to create new features beyond\nwhat is necessary for tidiness.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train <- read.csv(trainfile, stringsAsFactors = F)\ntest <- read.csv(testfile, stringsAsFactors = F)\nprice = train[,c(\"SalePrice\")]\n\nsimpledf <- rbind(train[,-81], test)\n\nsimpletrain <- deal_missing(simpledf)\nsdf = simpletrain\n\nsdftrain = sdf[1:1460,]\nsdftrain = cbind(sdftrain,price)\nsdftest = sdf[1461:2919,]\nsdftrain = sdftrain[,-1]\nid = sdftest[,1]\nsdftest = sdftest[,-1]\n\nstraincpy = sdftrain\nstraincpy[sapply(straincpy, is.factor)] <- lapply(straincpy[sapply(straincpy, is.factor)], as.numeric)\nstestcpy = sdftest\nstestcpy[sapply(stestcpy, is.factor)] <- lapply(stestcpy[sapply(stestcpy, is.factor)], as.numeric)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "And then we fit probably the simplest possible support vector machine, from\nthe user's point of view.  (A lot is going on under the hood.)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "model.svm <- svm(price ~ ., data = sdftrain, cost = 1)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "And we make predictions",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "price.svm = predict(model.svm, sdftest)\n\nsvmResult = cbind(Id = id, SalePrice = price.svm)\ncolnames(svmResult) = c(\"Id\",\"SalePrice\")",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Avearging Linear Results with SVM Results\n\nOne could try to do some sophisticated stacking, and maybe I will in the future,\nbut for now, just take the (logarithmic) average.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "final_answer <- exp (   ( log(sorted_result$SalePrice) + log(svmResult[,\"SalePrice\"]) ) / 2   )\nfinal_result <- data.frame( cbind( sorted_result$Id, final_answer ) )\nnames(final_result) <- c(\"Id\", \"SalePrice\")\nwrite.csv(final_result, file=\"kaggleSubmission5a.csv\", row.names=FALSE)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Ready to submit.",
      "metadata": {}
    }
  ]
}